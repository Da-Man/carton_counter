{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "carton_counter_v4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDGHnJWolwYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taWdiWX-YHeh",
        "colab_type": "code",
        "outputId": "a2866aac-3660-4e6f-c371-d7f27b2011d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz12STx2PJ9L",
        "colab_type": "code",
        "outputId": "10878dad-1dca-4541-a713-f7a989115716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import os\n",
        "import cv2 as cv\n",
        "import argparse\n",
        "import imutils\n",
        "import numpy as np\n",
        "from skimage.filters import threshold_local\n",
        "from google.colab.patches import cv_imshow\n",
        "from PIL import Image, ImageEnhance\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import splprep, splev\n",
        "import np_utils\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Dropout, LeakyReLU, Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, Convolution2D"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBm1NcETmYWw",
        "colab_type": "text"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_5D6Zds0RW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path = r'drive/My Drive/carton_counter/carton_test.jpg'\n",
        "\n",
        "train_dir = r'drive/My Drive/carton_counter/train/'\n",
        "train_dir_sqr = r'drive/My Drive/carton_counter/train/square/'\n",
        "train_dir_str = r'drive/My Drive/carton_counter/train/star/'\n",
        "train_dir_crc = r'drive/My Drive/carton_counter/train/circle/'\n",
        "train_dir_tri = r'drive/My Drive/carton_counter/train/triangle/'\n",
        "\n",
        "val_dir = r'drive/My Drive/carton_counter/val/'\n",
        "val_dir_sqr = r'drive/My Drive/carton_counter/val/square/'\n",
        "val_dir_str = r'drive/My Drive/carton_counter/val/star/'\n",
        "val_dir_crc = r'drive/My Drive/carton_counter/val/circle/'\n",
        "val_dir_tri = r'drive/My Drive/carton_counter/val/triangle/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8PRPkyrmZ-U",
        "colab_type": "text"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6x5poLwl4p0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 15\n",
        "img_size = 100 #size of image fed into model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg-779WFk5Ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "291ea452-f845-49b0-f927-3dc4275dfa91"
      },
      "source": [
        "print(len(os.listdir(train_dir+\"square\")))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQbmW0LemWDa",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpkfymc8WLG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten(dimData, images):\n",
        "  images = np.array(images)\n",
        "  images = images.reshape(len(images), dimData)\n",
        "  images = images.astype('float32')\n",
        "  images /=255\n",
        "  return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur7HtpZejwtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(train_dir, size):\n",
        "  folders, labels, images, class_lbls = os.listdir(train_dir), [], [], {}\n",
        "  for folder in folders:\n",
        "    print(\"Getting data from folder:\", folder, \"-----------------------------------------------\")\n",
        "    counter = 0\n",
        "    class_lbls[folder] = folders.index(folder)\n",
        "    for path in os.listdir(train_dir+folder):\n",
        "        if counter <= size:\n",
        "          img = cv.imread(train_dir+folder+'/'+path,0)\n",
        "          #cv_imshow(img)\n",
        "          images.append(cv.resize(img, (img_size, img_size)))\n",
        "          labels.append(folders.index(folder))\n",
        "          counter += 1\n",
        "  images = np.array(images)\n",
        "  labels = np.array(labels)\n",
        "  return images, labels, class_lbls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWhby_xfxXCX",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data for Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_CoaQS5UA4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99325189-6d64-40e4-e461-3db243b3c6af"
      },
      "source": [
        "images, labels, class_lbls = get_data(train_dir, 2000)\n",
        "print(len(images))\n",
        "print(len(labels))\n",
        "print(class_lbls)\n",
        "\n",
        "# Saving a loading numpy arrays from file\n",
        "\n",
        "np.save('drive/My Drive/carton_counter/np_arrays/images.npy', images)\n",
        "np.save('drive/My Drive/carton_counter/np_arrays/labels.npy', labels)\n",
        "\n",
        "#images = np.load(r'drive/My Drive/carton_counter/np_arrays/images.npy')\n",
        "#labels = np.load(r'drive/My Drive/carton_counter/np_arrays/labels.npy')\n",
        "\n",
        "\n",
        "#break data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, random_state = 0)\n",
        "print(\"Finished loading data for training -----------------------------------------------\")\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(y_train))\n",
        "print(len(x_test))\n",
        "print(len(y_test))\n",
        "\n",
        "#flatten data\n",
        "dataDim = np.prod(images[0].shape)\n",
        "train_data  = flatten(dataDim, x_train)\n",
        "test_data = flatten(dataDim, x_test)\n",
        "\n",
        "#change labels to categorical\n",
        "train_labels = np.array(y_train)\n",
        "test_labels = np.array(y_test)\n",
        "train_labels_one_hot = to_categorical(train_labels)\n",
        "test_labels_one_hot = to_categorical(test_labels)\n",
        "\n",
        "#determine the number of classes\n",
        "classes = np.unique(train_labels)\n",
        "nClasses  = len(classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting data from folder: square -----------------------------------------------\n",
            "Getting data from folder: circle -----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdEx6LfNmOIP",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RekJzmP-05Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step - 1 Convolution\n",
        "classifier.add(Convolution2D( 16, 3, 3, input_shape=(28, 28, 3), activation='relu'))\n",
        "# Step - 2 Pooling\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "classifier.add(Convolution2D(32, 3, 3, activation='relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Step - 3 Flattening\n",
        "classifier.add(Flatten())\n",
        "# Step - 4 Full connection -> First layer input layer then hidden layer\n",
        "# and last softmax layer\n",
        "classifier.add(Dense(56, activation='relu', kernel_initializer='uniform'))\n",
        "classifier.add(Dense(3, activation='softmax', kernel_initializer='uniform'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(\n",
        "    optimizer='adam', \n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_data, \n",
        "          train_labels_one_hot, \n",
        "          batch_size = 128, \n",
        "          epochs=25, \n",
        "          verbose=1,\n",
        "          validation_data=(test_data, test_labels_one_hot))\n",
        "\n",
        "#test model\n",
        "[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\n",
        "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
        "#save model\n",
        "model.save(r'drive/My Drive/carton_counter/models/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ugwXfp8mHBx",
        "colab_type": "text"
      },
      "source": [
        "# Predict Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVkYPUXCB7TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder, test_imgs = os.listdir(test_dir), []\n",
        "for f in folder:\n",
        "  img = cv.imread(test_dir+'/'+f,0)\n",
        "  #cv_imshow(img)\n",
        "  test_imgs.append(cv.resize(img, (img_size, img_size)))\n",
        "  #test_imgs.append(img)\n",
        "\n",
        "test_imgs_np = np.array(test_imgs)\n",
        "dataDim = np.prod(test_imgs[0].shape)\n",
        "test_imgs_np = flatten(dataDim, test_imgs_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6KGfUSVxqgs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98239c45-1df0-4e57-fbf5-41cb26251c10"
      },
      "source": [
        "# Predict Results\n",
        "print(class_lbls)\n",
        "print(prediction)\n",
        "\n",
        "model = load_model(r'drive/My Drive/carton_counter/models/model.h5')\n",
        "prediction = model.predict_classes(test_imgs_np)\n",
        "\n",
        "index = 0\n",
        "for i in prediction:\n",
        "  img = test_imgs[index]\n",
        "  cv_imshow(img)\n",
        "  print(\"This image belongs to class:\", class_lbls[i])\n",
        "  index += 1\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['square', 'circle', 'triangle', 'star']\n",
            "[1 2 3 0 0 3 3 1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAACTUlEQVR4nO3YT0hUQRwH8O/bNbSt\nEExSwbyIlAqFKHTIYAUF8RoIGXSQEJHAi4ii0IIgehC9CEHQyUO5/kG8eHBBD7FKHkQo8qKgBCl6\nKHBl88948FLob+b3e+/1BJnfcefN7/NmZue93xtH4f9HKADDIhaxiEUsYhGLWMR9ZAivf6cAOK2y\nTg6/7lr5BGDo/Pq2h415AkVx433OP/3yHj9LcLuykKVksvSyG6ye2/EN+Ugv3KMFf5C9V7rZDvce\neUdODu4aFrX8j2ek30AAiK57RD5XmRFEJzwhm5f+qS6GF2T4Bs9A4YZ75CnTAOpdI4n7bASDbpEY\n38CdH+6QMYEBzLpCUi9EiG7CaCQuM3RrTyNFQiSyT6aiX7+/hEiqg26j9PFMIYLM7+KRxNNSJL1O\ntfhZrdC3RYxw7YkcyU8Lp2tqWY78PCUark8F6SuyEgSSCAJ5HQRCVU8Uwn/zcoLYP8cuNiMOhZsx\n/MARG8/JZyr15FTZYmSGShXIZqRHMi3NdGuXSkUj+/dkRmSETOVbSaQrJDRrUlEnMZxmTSPtiypI\nhDSJfCq4AV1df/Xvk252mvoPubpm3XSp7R6e4XRp0xi+xCZ5xeobfRbT597sWzMRajEkMR8WxExG\ndN6UwoycDGRpjQLDXLEQpUbCunGkzAk4h2rtqePEItFWM3qTkYExEqXUFnFqULnN6c0+ufv6Et+O\n/v6h+DY6m3h9BceD6PsNAItfgIYyoLWY3VGCnEdyFagtEXWRIy7i6p/CFrGIRSxiEYtYxCK6OAMh\nHaeCjDAfnAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEBCA128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: circle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAACXklEQVR4nO2ZT0gUURzHvyslmQSF\nhxKE0JM3s0vSMW8rdPMkVupB8KRdooNeQju0Bw/NIfAYQhcpPCgEHdY/RYE7IoGQKB5CBbVQFmVX\ntMPusm3Ne+/7m2Ei5Pe7zM57v+/7vO/7zZud2U2cIf6o+gcMhShEIQpRiEIUohCFABdCaLbyV66J\nBGGcJG/2bsYOAd6OidIT0mfhvLc7sYPrmXqJ6EwYfkE2JNGInTRtFI5+C6+R1qSnyMAjgUgIWV8o\nfcpmBTJRQVbay8I3vEwEOen+bXZtx/FAMhVrME7rRDXpqjibpnWCSzh3UltxXvXlNqkUOHlSycDp\nA1bJQ1ZX/2z5SmvZ4p0+/FvbT2ppiB8wwTpyr9DL1RvQtveR07KQe0vsbIKCMzxoUHMbknOythbF\nB+dku8Ok7hhl9BSky8QgJ8klWWKA0DM1Sdo6F4kB3JDDvhlbd+4HQXFZPeh3DDBILLcrIeNg4OpS\n9Jp4roSfL/KuFJeTx84BAD/icj2/REAao0HIu+y+A2KtSXaSg9xZdyTYZjDMMZyXsc3J3DMW4grL\nBG7RgzTbnVggqcv8VCdDQl5eFKxH9bINYqzJ0YZ7I5cjZ30GMz6m+q0CBgDbl47RySsZw5pvdJIQ\nQur8BmOf0Uk63SmC7L0291leHbJHAOB9muUoNR/aQkDK4S3PA9jZt2c9HTHdsgUvQd9SuwCmzAnv\n7keHFGILgIf3nwO67i4ENIaClGK8ePyeKrcZr9Sof2YeLgJjaQCANxAXBABy2wCAG9VxQlxxfn7o\nVIhCFKIQhShEIQr5vyG/AEPDq0xRBfqHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEC18198>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: triangle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAABTklEQVR4nO3YMUvDQBgG4IsVF5cW\ndNFCp0LX4uTm/3BSJ8Ff4E9wUOjaQikt+APqoou4asXFzU2sDkp10LHGUdvyfebOr29Med/tcsk9\nx+VLuCSK3fQzBzCIECFChAgRIkSIODfvf8nd/fiR3uNe+U/I0/Fou999eZ08K3cYhtQe3OXFb1NI\nFhlp92wEl63qekYgHQSih8h4DhCIGhnZtvPlkXYjAOIgiN0Xftol/P+W61rtjcXkfJDFG3mg2Gq5\nPrSXfdo3HoLMznOyg0BWvAaqhyF++UQgp4MgZN0LeR8GIVUvREvaJbx8BkDsYodcIZB9BHKLQJRo\nyFIFgFS3AIhflC2BHTI8ASBxH4C4BgKRY4isiT0Bf4mENDemj7Q25b2zihTyb9+N0sJo5/lq4glE\n6j6x9aP2j4qJB/VDjJK1EiZChAgRIkSIEMkq8gWjUWQupN2PfAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEBCAB38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: star\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAACrElEQVR4nO2YPWgUURDH/5uIGND4\nFcGAhESREPXEkCiBELnGIl0QK0U0xRWCYGujnmiaqCkjaqUEJFVC0EMkcIVFJMUR9YhanOcViXfg\nBwa9D2PuWextNgn79s0c+w6LN82+nXkzv53dnfdm1xLQL3U1YBiIgRiIgRiIgRiIgdQUkszVABK6\nwPWw+M2dhdnjPA9+JmeB90wXNqT4BviqGxKbB17ohswBmF/SDAGAhQLTQfDkMQAgwXNiZvJ7AgBw\nlefFrJPcXgDANt5DYWZSsg+/7uiEHLEPYlYj5OMf1vTqIA8qtwszy9oghZQzWrirDTI8tTr8wPHj\nFGN6l+vWmNZUjJnv7ngpw3DcJNH/9Ci3h+tO9lcCNKshsoq/fkvtCwBo6Ok+7IyPdkomSW5j/CYR\nslauSYLJH/wZNmPPJBsiRrezEJsHpJH8XuFXEQaj/Yk8kG+dlM6TGbc/+8TxL8YfiW4Sojex4hdG\nVfGRDgIjogiiXFa+hJWMi6oY6rVrrs8f0TeuDEFYILP7/Bj9WXUE2ioclSFaZijuNEj8oCfCuhEn\nuRP3k8uekHqaM3U/+USbJhEaJMX9WKgGMubdZ5WTQUIkIkYDhEwPSwzEjZ4ESeUlhhyxoyS8geWQ\n1DsW2Css3klNxMabcCFZd3bdyen8663ueVORkgkF0uYGjQghxMS5LauK5wFBnu10IoajJVs1FDjk\nkhPwWNpR/Y06uoEyASJrU9dIpWHdveNQq6OqP3EA9mfEpLDUEdSZPLXndbzcoK80f74dBDmTRQDA\nUNepDfr7+RghB2Im9wBgxMOweBoAHhEyUUKSgBWKe9tWALQRIJSKH3wb9jZYV3pId0v5R+LbGAYb\npdZiV4GwaVbxe3CdZIrt+iEk+V//CxuIgRiIgRiIgRiIgdjyD0CGw/NFGRAdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEC18198>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: square\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAACj0lEQVR4nO2YPWhTURTH/69UUBdB\niQaxanVwcchUMgiCUrCDTla7C4JDaUAFMagFXQSpToUu9bNWoYNLqyXgIAUrDlF0yGAH6dO8BNqg\nJIIgHoc08Rlz3znn+V4pcu90P87//Dj3+16HEH/qWAWGhViIhViIhViIhVjI/wY55QyrIY72cueg\n681mpSZEdy2WtIo1OSYHAdyIGwIA4zFDni0AwEy8kFefAOB6rJCKBwCoekoKadLEimhYpaJwU3i2\nEl8ky7saqon4IvnxsZHTRaKC3GnmLqooqg1y09dmtpxQ6MLuXRc0xhrI8d+BYEoD6ZSbzn/wFWpn\n+9N49xwAOgY5pWRMZnJ1yDxnOHqmfb0xkpv3mtmi+pRqTYb1M6r3tP+FaTGaIHMZzRwFgJMl44o3\nbysPdIy+z0ZPARC6q0Ak8p7ZURCE6NI6IWPP9yA3wRC6fEyC6L79JdALA6GFIzyj80mwDxZCxTzb\nZbMcg4UQeTuDCBtTbBwSCC2NJ82QHl4vghBNGhnnJiODGClD30Ry2XkyYLjObd8gkv/brb4gtBPF\n6x5oL97iRthd5bn29UtlkVwGUV4Yw0GumhoeyiiiTjXLRyIbk8fmpkeRRdLjFyRS6/3FQkSRVPy3\nlcxY/kq/rzwdUSRZn/kgEZHrO2WyAgc6SF++SkRExXxz4WwTbPWKayq6dzeevclkLeXVH46l5Ui6\n631v3TL99o/qp5l69XnegwCy8jew72Vrw60TALCjwnoQ78LOdLq1augwALhVViuEbJ36uffv2tOU\nBTDAqvmBr+aAzKGj7Ruv1ZB7zbrg3yduFxBklCuwjyB+4BeB+/zsCEzq78EwaU3+3FmIhViIhViI\nhViIhawu5BfwtHU4Sops2wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEBCAB38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: square\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAABWElEQVR4nO3YvUoDQRQF4LNGxRQi\nFnYiaAgp1DKpbSz1AXwJ0TdIYSV2IS8gYmFhpT6AIQFbQUXSCIISCMQfkPyY2CSLInPj3d3coJ5T\nXXbmzjcLu8syXgeDz4iBQYQIESJEiBAhQgQYDdl/XQEAJGYHgdQOAdQOcP8EANjbliZ7mt/UXNUv\ns1/7UjdS3w/uZOuhV52+uObIW3Ujd2vd4rbefyNeQGSh3X9tP+/NMWHU/QhrDJSL0mhU78mVBVKw\nQMQQ+bvIZSMQsmuBrKsQ7AdClKlYIFKIfMuZBSJ96yNDOi0D5PXIAJFC5DchE/MGyNyObqXCcwBE\nm/yjASKECJGwObdAji2QEwvEnWEjyysGyJISyTpHwp4SdTO5uYp0pIh/xpEo9arYtNQgITOL5TqA\n5Pina+kMpjbUuxLPdy7eAGTi2kV1SEQZ9ntChAgRIkSIECHyX5APi7o425Ke2qwAAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEC18198>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: star\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAAA+klEQVR4nO3XMQrCQBAF0I3YBVtF\nEDsLjyD2nsJ76E1s7ey8hp0ExFqwMVpYi6BrZ6HMwMj4w+r/ZTbMy84Essli+H5qAIMIESJEiBAh\nQoQICKnLS/f+yVBoOlEWo5iRgQihLReKcru2ln3okZF5YSqkHdzTersybVGc1nFoQj4bfDM3IVrc\n2vUfg09rJxDkdwZ/LQDIWfmepNUuIkQsyACBlAhkh0D8khhyWAOQzQKAKPFDumMAsm8AkDBDIHKI\nJIUsWwAkd9tl1e1SD+puiFs0RP0190LckimNv71e6Fzeb+qtnqXEB9YQt1Q+eCJEiBAhQoQIESJ+\neQBHw4ZzFcCqmAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEBCAB38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: star\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAACR0lEQVR4nO3YPUgcQRQH8P8aRRTN\nBuSwEESjZyOEi6fmQITrRJQgNqn07CSdmCIhlbESPwpBrpCkCFdYWJoiKUIQUSRVEBsVCSZc8A6u\nEqMe6qQ14pt5b3fvRJnX7b2b+e3sW3Y+HIXCR0kRDItYxCIWsYhFLGIR71EqbbC796irYEj+JDUB\nAH9PSl2gdwEuH3F4i7uNwy+L135KPI885iqKEbsT4Zua9r3jNFZKcZBElHoMPalgkPzBsEM/h5KP\n6SCQbcPjDq37Rz6MmooanTvzibx/aDIAbPtCLuYYBICOvA9knmcAQz6QWi6CZc/IG/6Hre7YIzLJ\nJgDEdUhQn/q1pC5L8vsRkdLkaSR/foiQ/VlNkq6WMKr3Cl4T4GiVzlH6K7HSKB/Jmhj5SWaCXK3s\nSJFcWo6Q7xeFLHlAtqRIVm7QceeWqb8+FwE53CgCQsatIgONHjqLCZG2kAekWYjgmdxwK6lMYPMJ\n0Fn4+QToJjPkSL6KkVP5SOpbhUa4nMrQSPMLIfKazARYkyd0iqyJUsMiY5LuSDeSeIXACD/VJDUj\nUTN8o2pF048WOZ9iI7W6frSFfxCNVPGMhkFtWncHSqk4D9Fvgkyv8BiHaEnqqg7zsccnBqLdASnW\nscdLk9Fh6oGxLewLaTd27f3Gu+Ac4FxOl9HtU9/NHbCOolQ2c/OrXP42w2nOPFRDZhTf3N/XfnRW\n6Xnqv//xp9nk2fiVq5EGINbDaylAkNu8chGr4TeUIJ7j/i9TLWIRi1jEIhaxyN1G/gH5lewDg0UL\nSwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEC18198>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: circle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAACMklEQVR4nO3YPUhbURQA4PPU1JRA\ni0MXLRpEI6Ko1a0gRLAYqKCCqzh0aYciikMd1ODqVBWFunTsHgohoWBwSOngDyj+EaRgoVmEtHUx\n4nVNg+fcc16eEeGc7eXmnO/de9/Pvc8xcPdRUQZDEUUUUUQRRRRRxH1UCf+fPoe24N0hue9zGYA/\neQj4ARq+PvZXs1Md3uLueM+MFv3UNdvaylUMI1LRV7elPotecLKNYSCZgafYGYYmPUIOGqmB6Pnp\nBZKwXRpLpSPfmqyzah8yC7L53GqAM1Uasm0nAADe5UpA4uhlVRRv8u6RcaYB8ME1ssrtCEDVuksk\nzSYAIEIhXj3q48tUK+5HRErniZvhWhH2ZQNH8Ed9165QwV8a6JwkfgsNIlAkmZWWmsCbkGH85JMa\nUDGDzQnWk6u8GLlGUx7cuutLOZCztTIgaCiiyH/hOF4q2PNmWl4qGBM+u9zE6CDSgCJjDd7pKNJR\nLy1V+QJrIXZa0qnv3MFa7vs++eidgi9ktmpEhfqu0UrUWjgWEhg9GbwQuaqPPeEjC0QdehNUyzZe\n/yLK0FfXKbnzLQhfN3k+ZE/M/kseUktWse1+f7RzjLefS0LMGWMp+f4fXcP+RaK/xUL0Wkswvq1s\nResoY8iyv+YhxsQJY5i4CUWIyS4EArcJjwLZv4x05kc1ABg+OiweKBgfYaXyEdhIXSwWHIbD89xM\nAQJwmSw4CDWz80SI27jvN6MiiiiiiCKKKKIIHTelUGhnSMhTEQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F33AEBCAB38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "This image belongs to class: circle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcQ-4HoJcukC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total training images:\", len(x_train))\n",
        "print(\"Total validation images:\", len(y_train))\n",
        "print(\"Total test images:\", len(x_test))\n",
        "print(\"Total validation test images:\", len(y_test))\n",
        "print(x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1FoQFwOMHZe",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krLzM1oeMGzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(img_size, img_size ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOmj6obqOdbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(\n",
        "    x_train, \n",
        "    y_train,\n",
        "    batch_size = 128,\n",
        "    epochs = 1\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYNfSNF_OuAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3IeMt1OeSoe",
        "colab_type": "text"
      },
      "source": [
        "# Shape Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L1BilBU1MY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShapeDetector:\n",
        "  def __init__(self):\n",
        "    self.cartons = 0\n",
        "\n",
        "  def detect(self, c):\n",
        "    # initialize the shape name and approximate the contour\n",
        "    shape = \"unidentified\"\n",
        "    peri = cv.arcLength(c, True)\n",
        "    approx = cv.approxPolyDP(c, 0.015 * peri, True)\n",
        "\n",
        "    #epsilon = 0.1*cv.arcLength(c,True)\n",
        "    #approx = cv.approxPolyDP(c,epsilon,True)\n",
        "\n",
        "    # if the shape has 4 vertices, it is either a square or a rectangle\n",
        "    if len(approx) == 4:\n",
        "      # compute the bounding box of the contour and use the bounding box to compute the aspect ratio\n",
        "      (x, y, w, h) = cv.boundingRect(approx)\n",
        "      ar = w / float(h)\n",
        "\n",
        "      shape = \"square\" if ar >= 0.95 and ar <= 1.05 else \"rectangle\"\n",
        "      self.cartons += 1\n",
        "      shape += (\" \" + str(self.cartons))\n",
        "      \n",
        "      return shape, (x, y, w, h)\n",
        "    else:\n",
        "      return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l6nBEzpeWgZ",
        "colab_type": "text"
      },
      "source": [
        "# Auto Canny"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xJN0GEpeZ-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auto_canny(image, sigma=0.33):\n",
        "\t# compute the median of the single channel pixel intensities\n",
        "\tv = np.median(image)\n",
        " \n",
        "\t# apply automatic Canny edge detection using the computed median\n",
        "\tlower = int(max(0, (1.0 - sigma) * v))\n",
        "\tupper = int(min(255, (1.0 + sigma) * v))\n",
        "\tedged = cv.Canny(image, lower, upper)\n",
        " \n",
        "\t# return the edged image\n",
        "\treturn edged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyH0GI7zeb4L",
        "colab_type": "text"
      },
      "source": [
        "# Image Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzVuH_oEeelq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_img(image, task):\n",
        "  # Invert the Bits if the background is white\n",
        "  image = cv.bitwise_not(image)\n",
        "  cv_imshow(imutils.resize(image, width = 500))\n",
        "  # convert the image to grayscale\n",
        "  gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "  cv_imshow(imutils.resize(gray, width = 500))\n",
        "  # blur the image\n",
        "  #blurred = cv.GaussianBlur(gray, (5, 5), 0)\n",
        "  blurred = cv.bilateralFilter(gray, 11, 17, 17)\n",
        "  cv_imshow(imutils.resize(blurred, width = 500))\n",
        "\n",
        "  if task == \"thresh\":\n",
        "    # Contour Threshold \n",
        "    print(\"Threshhold-----------\")\n",
        "    thresh = cv.threshold(blurred, 100, 255, 0)[1]\n",
        "    cv_imshow(imutils.resize(thresh, width = 500))\n",
        "    # Find contours for thresholds \n",
        "    cnts = cv.findContours(thresh.copy(), cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = imutils.grab_contours(cnts)\n",
        "    cnts = sorted(cnts, key = cv.contourArea, reverse = True)[:30]\n",
        "    return cnts\n",
        "\n",
        "  elif task == \"edge\":\n",
        "    # Countour Edges\n",
        "    print(\"Edged-----------\")\n",
        "    #edged = auto_canny(blurred)\n",
        "    edged = cv.Canny(blurred, 30, 200)\n",
        "    cv_imshow(imutils.resize(edged, width = 500))\n",
        "    # Find contours for edges \n",
        "    cnts = cv.findContours(edged.copy(), cv.RETR_LIST, cv.CHAIN_APPROX_NONE)\n",
        "    cnts = imutils.grab_contours(cnts)\n",
        "    cnts = sorted(cnts, key = cv.contourArea, reverse = True)\n",
        "    return cnts\n",
        "\n",
        "  else: \n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePa6I4wjejvS",
        "colab_type": "text"
      },
      "source": [
        "# Detect Shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6HNfPm7eiXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the image and resize it to a smaller factor so thatthe shapes can be approximated better\n",
        "image = cv.imread(img_path)\n",
        "orig_image = image.copy()\n",
        "\n",
        "# Edge Analysis\n",
        "edged = translate_img(image, \"edge\")\n",
        "cv.drawContours(image, edged, -1, (0, 0, 0), 2)\n",
        "cv_imshow(imutils.resize(image, width = 500))\n",
        "\n",
        "# Thresh Analysis\n",
        "thresh = translate_img(image, \"thresh\")\n",
        "sd = ShapeDetector()\n",
        "\n",
        "# loop over the contours\n",
        "for c in thresh:\n",
        "  result = sd.detect(c)\n",
        "  if result != None:\n",
        "    shape = result[0]\n",
        "    (x, y, w, h) = result[1]\n",
        "    cv.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    cv.putText(image, shape, (x, y), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "\n",
        "# show the output image\n",
        "print(\"Result-----------\")    \n",
        "cv_imshow(imutils.resize(image, width = 800))\n",
        "\n",
        "#print(\"Smoothened Contours-----------\")\n",
        "#img = image\n",
        "#smooth_cnts = smoothen_cnts(img, cnts)\n",
        "#cv_imshow(imutils.resize(image, width = 800))\n",
        "print(\"Number of Cartons:\", sd.cartons)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}